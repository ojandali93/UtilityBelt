# UtilityBelt

Like a handymans utility belt, this repository consist of a wide variety of functions and classes that can be used for all types of data importing, analysis, and more.

All of the functions and methods included in this repo are written in python and meant to be used with any python based data science. 

You can access each file by clicking any fo the links below:

<h2>Helper / General use functions</h2>
<ul>
  <a><li>ReadInData:</li></a>
  <li>Read in any type of data into a dataset / dataframe structure</li>
  <a><li>GettingInfoAboutDataset</li></a>
  <li>Generates, prints, and returns different type of data about the dataset and all of its contents</li>
  <a><li>EliminateNullValues</li></a>
  <li>Allows you to eliminate all null values from a dataset. You can pass specific columns as arguments that will manipulate that specific column. You can also submit a list of columns as well. The third optional parameter is a % threshold. If the amount of null values is above the threshold, it will eliminate null values</li>
  <a><li>CreateSubsetFromDataset</li></a>
  <li>Generate a simple subset from a dataset based on the argument that is passed through.</li>
  <a><li>AlterNullValues</li></a>
  <li>You can replace all null values with an argument that you pass into the function as an argument</li>
  <a><li>EncodeDataFromDataset</li></a>
  <li>Easily encod your data from easier manipulation and analysis</li>
</ul>

<h2>Different classes / engines</h2>
<ul>
  <a><li>AnalysisEngine</li></a>
  <a><li>VisualizationEngine</li></a>
</ul>

<h2>Glossery</h2>
<ul>
  <a><li>exploratory testing: manually evaluating specific features and use cases</li></a>
  <a><li>manual testing: automated testing practices in which tests are executed and assessed by the code itself.</li></a>
  <a><li>unit tests: test which evaluate a single component or script within a closed context</li></a>
  <a><li>integration tests: tests multiple components or scripts within either closed or live contexts/environments</li></a>
  <a><li>catch cases: discrete lines of code that account for those cases/conditions</li></a>
  <a><li>data validation / error handling: hey provide us with explicit means of correcting for and handling sources of error, bias, incorrect data, and other noise in our system by filtering them out or reporting metadata back to the user/developer.
</li></a>
  <a><li>Test-Driven Development (TDD): ensures that as we build out a platform, improve on scripts, and contribute to a pipeline that we ensure our workflow is stable and error-free to the best of our ability.</li></a>
  <a><li>inferential statistics: gives us the able to navigate between sampling spaces and population spaces using carefully engineered assumptions and assessments.</li></a>
  <a><li>Z-Scores: allows us to to understand proportions of data contained between two values across the parent distribution.
</li></a>
  <a><li>Standard Normal Distribution: special form of this distribution, allowing any data distribution to be interpreted in a single context, assuming that the mean is zero and the standard deviation is one.
</li></a>
  <a><li>Central Limit Theorem: allows us to sample data from any population and do so enough times to produce a reliable normal distribution, given specific parameters/constraints.</li></a>
  <a><li>Confidence intervals: they allow us to state a measure that describes how likely a resampled estimate/value will fall within a specific target range. 
</li></a>
</ul>